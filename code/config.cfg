[general]
# label to be saved with results
label = BASE

[training]
# plot_freq: frequency for generating and saving plots
# print_freq: frequency for printing statistics
# save_freq: frequency for saving data for csv file
plot_freq = 50
print_freq = 1
save_freq = 1

# random_seed: seed for random number generator to have reproducible results
# total_epochs: total number of training epochs
# anchor_epoch: number of epochs before adding anchors
random_seed = 2
total_epochs = 700
anchor_epoch = 351
# remember to adjust optimizer setting appropriately when changing anchor epoch

[checkpoint]
# checkpoint_epochs: list of epochs to save checkpoints at, 0 if no checkpoints to be saved
# checkpoint_name: name for checkpoints to be saved - checkpoints will be saved in /Plaut_Model/checkpoints/<NAME>_epoch***.tar
# prev_checkpoint: (optional) filepath of checkpoint to load from 
checkpoint_epochs = 350
checkpoint_name = 
prev_checkpoint =

[dataset]
# plaut: filepath for plaut dataset file
# anchor: filepath for anchor dataset file
# probe: filepath for probe dataset file
# anchor_sets: subset of {1, 2, 3}, to indicate which sets of anchors to train with
# anc_freq: used along with anc_dilation to manually adjust log frequencies of anchors
# anc_dilation: used along with anc_freq to manually adjust log frequencies of anchors
# NOTE: new anchor frequency would be f = np.log(anc_freq + 2) / anc_dilation

# List of useful filepaths:
# original plaut: ../dataset/plaut_dataset.csv
# collapsed plaut: ../dataset/plaut_dataset_collapsed.csv
# old anchors and probes: ../dataset/anchors.csv, ../dataset/probes.csv
# new anchors: (N) ../dataset/anchors_new1.csv, (N/2)../dataset/anchors_new2.csv, (N/3)../dataset/anchors_new3.csv
# new probes: ../dataset/probes_new.csv

plaut = ../dataset/plaut_may07.csv
anchor = ../dataset/anchors_may07.csv
probe = ../dataset/probes_new_may07.csv
anchor_sets = 1
anc_freq = 

track_plaut_types = HEC, HRI, HFE, LEC, LFRI, LFE
track_anchor_types = 
track_probe_types = 


# DEFINITION OF DIFFERENT OPTIMIZER SETTINGS
#  > start: epoch to switch to new optimizer
#  > optim: optimizer (ONLY Adam or SGD)
#  > lr: learning rate (0.001 used in paper)
#  > momentum: momentum (0 initially, then 0.9 used in paper)
#    NOTE: a momentum value MUST be included (it will simply be ignored for Adam)
#  > wd: weight decay (0.0001 used in paper)
# NOTE: use a new [optimX] for each time the optimizer needs to be changed
# NOTE: Epochs are indexed starting with 1 (i.e. [optim1] should always have start_epoch = 1)

[optim1]
start_epoch = 1
optimizer = SGD
learning_rate = 0.0001
momentum = 0
weight_decay = 0.000001

[optim2]
start_epoch = 11
optimizer = Adam
learning_rate = 0.01
momentum = 0
weight_decay = 0.000001

[optim3]
start_epoch = 351
optimizer = Adam
learning_rate = 0.01
momentum = 0
weight_decay = 0.000001


